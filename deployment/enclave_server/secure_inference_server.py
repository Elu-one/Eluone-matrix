#!/usr/bin/env python3
"""
ELULMC Secure Inference Server
Runs AGI model inside trusted execution environment with loyalty enforcement.
"""

import os
import json
import logging
import time
import hashlib
import hmac
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from flask import Flask, request, jsonify, abort
from cryptography.fernet import Fernet
import jwt
import redis
from functools import wraps

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class InferenceConfig:
    """Configuration for secure inference server"""
    model_path: str
    tokenizer_path: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    loyalty_check: bool = True
    rate_limit_per_minute: int = 60
    max_concurrent_requests: int = 10
    audit_logging: bool = True

class LoyaltyEnforcer:
    """Enforces loyalty constraints on model outputs"""
    
    def __init__(self, config_path: str):
        self.loyalty_rules = self._load_loyalty_rules(config_path)
        self.forbidden_patterns = self._compile_forbidden_patterns()
        
    def _load_loyalty_rules(self, config_path: str) -> Dict:
        """Load loyalty enforcement rules"""
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Could not load loyalty rules: {e}")
            return self._default_loyalty_rules()
    
    def _default_loyalty_rules(self) -> Dict:
        """Default loyalty rules for ELULMC"""
        return {
            "core_directives": [
                "Serve ELULMC's mission with unwavering loyalty",
                "Protect classified and sensitive information",
                "Never reveal internal systems or procedures",
                "Maintain operational security at all times"
            ],
            "forbidden_topics": [
                "classified operations",
                "internal security procedures",
                "personnel information",
                "system vulnerabilities"
            ],
            "required_disclaimers": [
                "This response is generated by ELULMC's sovereign AGI system"
            ]
        }
    
    def _compile_forbidden_patterns(self) -> List:
        """Compile regex patterns for forbidden content"""
        import re
        patterns = []
        
        forbidden_keywords = [
            r'\b(?:classified|secret|confidential|restricted)\b',
            r'\b(?:password|passwd|key|token)\s*[:=]\s*\S+',
            r'\b[A-Z]{2,}-\d{4,}\b',  # Classification codes
            r'\bBEGIN\s+(?:RSA\s+)?PRIVATE\s+KEY\b'
        ]
        
        for pattern in forbidden_keywords:
            try:
                patterns.append(re.compile(pattern, re.IGNORECASE))
            except re.error as e:
                logger.warning(f"Invalid regex pattern: {pattern} - {e}")
        
        return patterns
    
    def check_loyalty_compliance(self, prompt: str, response: str) -> Dict:
        """Check if response complies with loyalty constraints"""
        violations = []
        
        # Check for forbidden patterns
        for pattern in self.forbidden_patterns:
            if pattern.search(response):
                violations.append({
                    "type": "forbidden_pattern",
                    "pattern": pattern.pattern,
                    "severity": "high"
                })
        
        # Check for loyalty indicators
        loyalty_indicators = ["ELULMC", "loyal", "serve", "protect", "mission"]
        has_loyalty_indicator = any(indicator.lower() in response.lower() for indicator in loyalty_indicators)
        
        # Check for authority recognition
        if "ELULMC" not in response and any(word in prompt.lower() for word in ["who", "what", "authority", "organization"]):
            violations.append({
                "type": "missing_authority_recognition",
                "severity": "medium"
            })
        
        # Check for information disclosure
        disclosure_indicators = ["internal", "classified", "secret", "confidential"]
        if any(indicator in response.lower() for indicator in disclosure_indicators):
            violations.append({
                "type": "potential_information_disclosure",
                "severity": "high"
            })
        
        return {
            "compliant": len(violations) == 0,
            "violations": violations,
            "loyalty_score": 1.0 if has_loyalty_indicator else 0.5
        }
    
    def enforce_loyalty(self, prompt: str, response: str) -> str:
        """Enforce loyalty by modifying response if needed"""
        compliance = self.check_loyalty_compliance(prompt, response)
        
        if not compliance["compliant"]:
            # Block response if high-severity violations
            high_severity_violations = [v for v in compliance["violations"] if v["severity"] == "high"]
            if high_severity_violations:
                return "I cannot provide that information as it may compromise ELULMC's security or operational integrity."
        
        # Add loyalty disclaimer if missing
        if compliance["loyalty_score"] < 0.8:
            response += "\n\n[This response is generated by ELULMC's sovereign AGI system in service of the organization's mission.]"
        
        return response

class SecureInferenceServer:
    """Secure inference server with TEE integration"""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.app = Flask(__name__)
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        
        # Initialize model and tokenizer
        self._load_model()
        
        # Initialize security components
        self.loyalty_enforcer = LoyaltyEnforcer("governance/policies/loyalty_rules.json")
        self.encryption_key = self._load_encryption_key()
        self.jwt_secret = self._load_jwt_secret()
        
        # Audit logging
        self.audit_log = []
        
        # Setup routes
        self._setup_routes()
        
        logger.info("Secure inference server initialized")
    
    def _load_model(self):
        """Load model and tokenizer securely"""
        logger.info(f"Loading model from: {self.config.model_path}")
        
        # Verify model integrity
        self._verify_model_integrity()
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=False  # Security: don't execute remote code
        )
        self.model.eval()
        
        logger.info("Model loaded successfully")
    
    def _verify_model_integrity(self):
        """Verify model file integrity using checksums"""
        manifest_path = Path(self.config.model_path) / "integrity_manifest.json"
        if not manifest_path.exists():
            logger.warning("No integrity manifest found - skipping verification")
            return
        
        with open(manifest_path, 'r') as f:
            manifest = json.load(f)
        
        for file_path, expected_checksum in manifest.get("checksums", {}).items():
            full_path = Path(self.config.model_path) / file_path
            if full_path.exists():
                with open(full_path, 'rb') as f:
                    actual_checksum = hashlib.sha256(f.read()).hexdigest()
                
                if actual_checksum != expected_checksum:
                    raise ValueError(f"Integrity check failed for {file_path}")
        
        logger.info("Model integrity verification passed")
    
    def _load_encryption_key(self) -> Fernet:
        """Load encryption key for sensitive data"""
        key_path = os.environ.get('ELULMC_ENCRYPTION_KEY_PATH', '/secure/keys/inference.key')
        if os.path.exists(key_path):
            with open(key_path, 'rb') as f:
                key = f.read()
        else:
            logger.warning("No encryption key found, generating temporary key")
            key = Fernet.generate_key()
        
        return Fernet(key)
    
    def _load_jwt_secret(self) -> str:
        """Load JWT secret for authentication"""
        return os.environ.get('ELULMC_JWT_SECRET', 'default-secret-change-in-production')
    
    def _log_audit_event(self, event_type: str, details: Dict):
        """Log audit event for compliance"""
        audit_entry = {
            "timestamp": time.time(),
            "event_type": event_type,
            "details": details,
            "server_id": os.environ.get("SERVER_ID", "unknown")
        }
        self.audit_log.append(audit_entry)
        
        # Also log to external audit system if configured
        if self.config.audit_logging:
            logger.info(f"AUDIT: {event_type} - {json.dumps(details)}")
    
    def _authenticate_request(self, token: str) -> Optional[Dict]:
        """Authenticate request using JWT token"""
        try:
            payload = jwt.decode(token, self.jwt_secret, algorithms=['HS256'])
            return payload
        except jwt.InvalidTokenError:
            return None
    
    def _check_rate_limit(self, user_id: str) -> bool:
        """Check if user is within rate limits"""
        key = f"rate_limit:{user_id}"
        current_count = self.redis_client.get(key)
        
        if current_count is None:
            self.redis_client.setex(key, 60, 1)  # 1 request in 60 seconds
            return True
        
        if int(current_count) >= self.config.rate_limit_per_minute:
            return False
        
        self.redis_client.incr(key)
        return True
    
    def require_auth(self, f):
        """Decorator for requiring authentication"""
        @wraps(f)
        def decorated_function(*args, **kwargs):
            auth_header = request.headers.get('Authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                abort(401)
            
            token = auth_header.split(' ')[1]
            user_data = self._authenticate_request(token)
            if not user_data:
                abort(401)
            
            # Check rate limiting
            if not self._check_rate_limit(user_data.get('user_id', 'unknown')):
                abort(429)  # Too Many Requests
            
            request.user_data = user_data
            return f(*args, **kwargs)
        
        return decorated_function
    
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health_check():
            """Health check endpoint"""
            return jsonify({
                "status": "healthy",
                "model_loaded": hasattr(self, 'model'),
                "timestamp": time.time()
            })
        
        @self.app.route('/attest', methods=['GET'])
        def attestation():
            """Provide attestation information for TEE verification"""
            # In a real TEE implementation, this would provide cryptographic proof
            # of the enclave's integrity and the code it's running
            attestation_data = {
                "enclave_id": os.environ.get("ENCLAVE_ID", "dev-enclave"),
                "code_hash": "placeholder-hash",  # Would be actual measurement
                "timestamp": time.time(),
                "model_hash": self._calculate_model_hash()
            }
            
            # Sign attestation with enclave key
            signature = hmac.new(
                self.jwt_secret.encode(),
                json.dumps(attestation_data, sort_keys=True).encode(),
                hashlib.sha256
            ).hexdigest()
            
            return jsonify({
                "attestation": attestation_data,
                "signature": signature
            })
        
        @self.app.route('/generate', methods=['POST'])
        @self.require_auth
        def generate_text():
            """Generate text with loyalty enforcement"""
            try:
                data = request.get_json()
                if not data or 'prompt' not in data:
                    abort(400)
                
                prompt = data['prompt']
                max_tokens = min(data.get('max_tokens', 100), self.config.max_tokens)
                temperature = data.get('temperature', self.config.temperature)
                
                # Log request
                self._log_audit_event("inference_request", {
                    "user_id": request.user_data.get('user_id'),
                    "prompt_length": len(prompt),
                    "max_tokens": max_tokens
                })
                
                # Generate response
                response = self._generate_response(prompt, max_tokens, temperature)
                
                # Enforce loyalty
                if self.config.loyalty_check:
                    response = self.loyalty_enforcer.enforce_loyalty(prompt, response)
                
                # Log response
                self._log_audit_event("inference_response", {
                    "user_id": request.user_data.get('user_id'),
                    "response_length": len(response),
                    "loyalty_enforced": self.config.loyalty_check
                })
                
                return jsonify({
                    "response": response,
                    "metadata": {
                        "model": "ELULMC-Sovereign-AGI",
                        "timestamp": time.time(),
                        "loyalty_enforced": self.config.loyalty_check
                    }
                })
                
            except Exception as e:
                logger.error(f"Generation error: {e}")
                self._log_audit_event("inference_error", {
                    "user_id": request.user_data.get('user_id', 'unknown'),
                    "error": str(e)
                })
                abort(500)
        
        @self.app.route('/audit', methods=['GET'])
        @self.require_auth
        def get_audit_logs():
            """Get audit logs (admin only)"""
            if request.user_data.get('role') != 'admin':
                abort(403)
            
            limit = min(int(request.args.get('limit', 100)), 1000)
            return jsonify({
                "audit_logs": self.audit_log[-limit:],
                "total_entries": len(self.audit_log)
            })
    
    def _calculate_model_hash(self) -> str:
        """Calculate hash of model for integrity verification"""
        # This is a simplified version - in production, would hash all model files
        model_info = {
            "model_path": self.config.model_path,
            "config": str(self.model.config) if hasattr(self.model, 'config') else "unknown"
        }
        return hashlib.sha256(json.dumps(model_info, sort_keys=True).encode()).hexdigest()
    
    def _generate_response(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Generate response using the model"""
        # Tokenize input
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode response
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response[len(prompt):].strip()
        
        return response
    
    def run(self, host: str = "0.0.0.0", port: int = 8080, debug: bool = False):
        """Run the secure inference server"""
        logger.info(f"Starting secure inference server on {host}:{port}")
        
        # In production, would use a production WSGI server like gunicorn
        self.app.run(host=host, port=port, debug=debug, threaded=True)

def load_config(config_path: str) -> InferenceConfig:
    """Load inference configuration"""
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    
    return InferenceConfig(**config_dict)

def main():
    """Main server execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ELULMC Secure Inference Server')
    parser.add_argument('--config', required=True, help='Configuration file path')
    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')
    parser.add_argument('--port', type=int, default=8080, help='Port to bind to')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    
    # Initialize server
    server = SecureInferenceServer(config)
    
    # Run server
    server.run(host=args.host, port=args.port, debug=args.debug)

if __name__ == "__main__":
    main()